= Running Highly Available Hazelcast Clusters
:description: High Availability Mode guarantees that even if the Kubernetes node or whole availability zone is down, and all related Hazelcast members are terminated, you will not experience any data loss.

In Hazelcast, you can enable and configure partition groups to force Hazelcast members to store backups of a data partition inside a member located in a different availability zone (ZONE_AWARE) or on a different Kubernetes node (NODE_AWARE). See xref:hazelcast:clusters:partition-group-configuration.adoc[Partition Group Configuration] for more details. In the Hazelcast Platform Operator, you can use these partition groups with High Availability Mode to guarantee that even if the Kubernetes node or whole availability zone is down, and all related Hazelcast members are terminated, you will not experience data loss.

When using the ZONE_AWARE and NODE_AWARE partition grouping, a Hazelcast cluster spanning multiple availability zones and nodes should have an equal number of members in each availability zones and node. Otherwise, it results in uneven partition distribution among the members.

In the Hazelcast Platform Operator, you can configure the type of partition group and required Kubernetes scheduling policy(`topologySpreadConstraints`) to distribute members across availability zones and nodes automatically by modifying the `highAvailabilityMode` parameter.


== Configuring High Availability Mode 

Below are the configuration options for the High Availability Mode feature.

[cols="20%m,80%a"]
|===
|Field|Description

|`highAvailabilityMode`
|Configuration for partition groups and Kubernetes scheduling policy::

  - `NODE`: the partition-group is configured as NODE_AWARE and `topologySpreadConstraints` is added into the statefulset:

[source,yaml]
```
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
          matchLabels:
            app.kubernetes.io/name: hazelcast
            app.kubernetes.io/instance: hazelcast
            app.kubernetes.io/managed-by: hazelcast-platform-operator
```

  - `ZONE`: the partition-group is configured as ZONE_AWARE and `topologySpreadConstraints` is added into the statefulset: 
[source,yaml]
```
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
          matchLabels:
            app.kubernetes.io/name: hazelcast
            app.kubernetes.io/instance: hazelcast
            app.kubernetes.io/managed-by: hazelcast-platform-operator
```

|===

=== Example Configuration

[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-high-availability-mode.yaml[]
----